AWSTemplateFormatVersion: '2010-09-09'
Description: >
  HTW Berlin - Serverless AI Pipeline.
  Features: S3 Ingestion, Lambda Processing, Amazon Translate, Amazon Comprehend, and SQL Analytics via AWS Glue/Athena.

Resources:
  # ==========================================
  # 1. STORAGE LAYER (S3 Buckets)
  # ==========================================
  
  # Input Bucket: Stores raw text files uploaded by the user.
  # Triggers the Lambda function upon file upload.
  RawDataBucket:
    Type: AWS::S3::Bucket
    DependsOn: LambdaInvokePermission
    Properties:
      BucketName: !Sub 'htw-raw-data-${AWS::AccountId}-${AWS::Region}'
      NotificationConfiguration:
        LambdaConfigurations:
          - Event: s3:ObjectCreated:*
            Function: !GetAtt AIProcessorFunction.Arn

  # Output Bucket: Stores the processed JSON results.
  # acts as the Data Lake storage for Athena analysis.
  ProcessedDataBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub 'htw-processed-data-${AWS::AccountId}-${AWS::Region}'

  # ==========================================
  # 2. SECURITY LAYER (IAM Roles)
  # ==========================================

  # Role for Lambda: Grants permission to access S3, Translate, and Comprehend services.
  AIProcessorRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: AI-Service-Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:* # Read/Write files
                  - comprehend:* # Sentiment Analysis
                  - translate:* # Language Translation
                  - logs:* # CloudWatch Logging
                Resource: '*'

  # Role for AWS Glue: Grants permission to crawl S3 buckets and update the Data Catalog.
  GlueCrawlerRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: glue.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      Policies:
        - PolicyName: S3AccessForGlue
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: s3:*
                Resource: '*'

  # ==========================================
  # 3. COMPUTE LAYER (Lambda Function)
  # ==========================================

  # The Core Logic: Handles Translation, Sentiment Analysis, and Data Transformation.
  AIProcessorFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: HTW-Customer-Insight-AI
      Handler: index.lambda_handler
      Role: !GetAtt AIProcessorRole.Arn
      Runtime: python3.9
      Timeout: 30
      Code:
        ZipFile: |
          import json
          import boto3
          import os

          # Initialize AWS SDK Clients
          s3 = boto3.client('s3')
          comprehend = boto3.client('comprehend')
          translate = boto3.client('translate')

          def lambda_handler(event, context):
              print("Pipeline Started: Translation & Sentiment Analysis")
              
              # 1. Parse Event Data (Get Bucket and File Name)
              try:
                  record = event['Records'][0]
                  src_bucket = record['s3']['bucket']['name']
                  src_key = record['s3']['object']['key']
                  print(f"Processing File: {src_bucket}/{src_key}")
                  
                  # 2. Read File Content
                  obj = s3.get_object(Bucket=src_bucket, Key=src_key)
                  original_text = obj['Body'].read().decode('utf-8')
              except Exception as e:
                  print(f"Error reading file: {str(e)}")
                  return {'status': 'error'}

              # 3. AMAZON TRANSLATE: Convert text to English
              print("Step 1: Translating text...")
              translation_response = translate.translate_text(
                  Text=original_text,
                  SourceLanguageCode='auto', # Automatically detect language (e.g., DE, TR)
                  TargetLanguageCode='en'    # Translate to English
              )
              translated_text = translation_response['TranslatedText']
              source_lang = translation_response['SourceLanguageCode']
              print(f"Detected Language: {source_lang}")

              # 4. AMAZON COMPREHEND: Analyze Sentiment
              print("Step 2: Analyzing sentiment...")
              sentiment_response = comprehend.detect_sentiment(Text=translated_text, LanguageCode='en')
              sentiment = sentiment_response['Sentiment']      # POSITIVE, NEGATIVE, etc.
              score = sentiment_response['SentimentScore']     # Confidence scores

              # 5. Prepare Data for Analytics (Flattened JSON Structure)
              # This structure is optimized for AWS Athena SQL queries.
              analysis_result = {
                  'original_text': original_text,
                  'detected_language': source_lang,
                  'translated_english': translated_text,
                  'sentiment': sentiment,
                  'confidence_positive': score['Positive'],
                  'confidence_negative': score['Negative'],
                  'confidence_neutral': score['Neutral'],
                  'author': 'Yigit Peker',
                  'project': 'HTW Berlin Serverless Project'
              }

              # 6. Save Result to Processed Bucket
              target_bucket = src_bucket.replace('raw-data', 'processed-data')
              target_key = 'AI_RESULT_' + src_key + '.json'
              
              s3.put_object(
                  Bucket=target_bucket,
                  Key=target_key,
                  Body=json.dumps(analysis_result) # Saving as single-line JSON for Athena
              )
              
              print(f"Success: Data saved to {target_bucket}/{target_key}")
              return {'status': 'success', 'mood': sentiment}

  # Permission to allow S3 to trigger the Lambda function
  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref AIProcessorFunction
      Action: lambda:InvokeFunction
      Principal: s3.amazonaws.com
      SourceArn: !Sub 'arn:aws:s3:::htw-raw-data-${AWS::AccountId}-${AWS::Region}'

  # ==========================================
  # 4. ANALYTICS LAYER (Glue & Athena)
  # ==========================================

  # Glue Database: Acts as a logical container for the data tables.
  AnalyticsDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: htw_customer_insights_db

  # Glue Crawler: Scans S3 JSON files and automatically creates the SQL table schema.
  DataCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: HTW-Data-Crawler
      Role: !GetAtt GlueCrawlerRole.Arn
      DatabaseName: !Ref AnalyticsDatabase
      Targets:
        S3Targets:
          - Path: !Ref ProcessedDataBucket
      SchemaChangePolicy:
        UpdateBehavior: "UPDATE_IN_DATABASE"
        DeleteBehavior: "DEPRECATE_IN_DATABASE"
