import boto3
import json
import os

# AWS AI and S3 Clients
s3 = boto3.client('s3')
translate = boto3.client('translate')
comprehend = boto3.client('comprehend')

def lambda_handler(event, context):
    # Get bucket and file information from the S3 trigger event
    source_bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    target_bucket = os.environ['TARGET_BUCKET']
    
    # 1. Read the raw file from S3
    response = s3.get_object(Bucket=source_bucket, Key=key)
    content = response['Body'].read().decode('utf-8')
    
    # 2. Split the content into individual lines (reviews)
    lines = [line.strip() for line in content.split('\n') if line.strip()]
    
    processed_results = []
    
    # 3. Process each line one by one
    for line in lines:
        try:
            # Step A: Detect language and translate to English automatically
            trans_res = translate.translate_text(
                Text=line,
                SourceLanguageCode='auto',
                TargetLanguageCode='en'
            )
            translated_text = trans_res['TranslatedText']
            detected_lang = trans_res['SourceLanguageCode']
            
            # Step B: Perform Sentiment Analysis on the English text
            comp_res = comprehend.detect_sentiment(
                Text=translated_text,
                LanguageCode='en'
            )
            
            # Step C: Collect individual results for this specific line
            processed_results.append({
                "original_text": line,
                "detected_language": detected_lang,
                "translated_english": translated_text,
                "sentiment": comp_res['Sentiment'],
                "confidence_score": comp_res['SentimentScore'][comp_res['Sentiment'].title()],
                "project": "HTW Berlin Serverless Project",
                "author": "Yigit Peker"
            })
        except Exception as e:
            print(f"Error processing line: {str(e)}")
            continue

    # 4. Save results in JSON Lines format so Athena can parse each row separately
    output_key = f"AI_RESULT_{key}.json"
    json_output = "\n".join([json.dumps(res) for res in processed_results])
    
    s3.put_object(
        Bucket=target_bucket,
        Key=output_key,
        Body=json_output
    )
    
    return {
        "status": "success", 
        "total_lines_processed": len(processed_results)
    }
